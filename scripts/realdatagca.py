import os

import numpy as np
import pandas as pd
from loguru import logger
from sklearn import linear_model
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.tree import DecisionTreeClassifier
from verticapy import vDataFrame
from verticapy.connect import connect
from verticapy.connect import new_connection

from lrtree import Lrtree
from lrtree.discretization import traitement_train
from lrtree.discretization import traitement_val
from lrtree.fit import _fit_parallelized

conn_info = {"host": os.environ.get("IP_VERTICA"),
             "port": 5433,
             "user": "LK1ASASVIEW",
             "password": os.environ.get("MDP_VERTICA"),
             "database": "LKPRD2",
             # autogenerated session label by default,
             "session_label": "some_label",
             # default throw error on invalid UTF-8 results
             "unicode_error": "strict",
             # SSL is disabled by default
             "ssl": False,
             # using server-side prepared statements is disabled by default
             "use_prepared_statements": False,
             # connection timeout is not enabled by default
             # 5 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
             "connection_timeout": 100000}

new_connection(conn_info, name="MADERE")
connect("MADERE")


if __name__ == "__main__":
    # Variables utilisées par le retail (tous segments cumulés)
    Used = ["DAV_Null_EHB_DAV", "DAV_Null_SLD_MOY_CREDITEUR_M", "DAV_Null_SOLDE_MOYEN_FLUX_12M",
            "DAV_Null_SOLDE_MOYEN_M", "DAV_Null_SOLDE_MINIMUM_12M", "DAV_Null_FLX_DBT_NBOPE_DEB_12",
            "DAV_Null_MNT_REFUS_PAIEMENT_M", "DAV_Null_MNT_REFUS_PAIEMT_12M", "DAV_Null_NB_REFUS_PAIEMT_3M",
            "DAV_Null_NB_REFUS_PAIEMT_6M", "DAV_Null_NB_REFUS_PAIEMT_M", "DAV_Null_NBPREL_ORG_FINANC_6M",
            "DAV_Null_NB_TOT_JOURS_DEP_3M", "DAV_Null_NB_TOT_JOURS_DEP_M", "DAV_Null_NB_TOT_JOURS_DEP_12M",
            "DAV_Null_NB_TOT_JOURS_DEP_6M", "DAV_Null_NB_TOT_PMT_CARTE_12M", "DAV_Null_NBTOT_JOURS_DEBIT_6M",
            "DAV_Null_NB_OPE_DEBIT_12M", "DAV_Null_NB_CARTES_DAV", "DAV_Null_NBJOURS_DPS_DEPASS",
            "DAV_Null_CPT_JOURS_CONS_DEP_M", "INTERETS_DAV_FLUX", "INTERETS_DEBIT_12M_TIERS",
            "CRED_Null_MAXJOUR_CONS_RET_12M", "CRED_Null_MNT_TOT_IMPAYE_CON", "CRED_Null_NB_JOURS_MAX_RETARD",
            "CRED_Null_NB_JOURS_CONS_RETARD", "ENGAGEMENTS_HORS_BILAN_GR_Calc", "EPARGNE_TOTALE", "EPARGNE_LOGEMENT_GR",
            "EPARGNE_LIVRET_GR", "ENCOURS_RETARD_INF90", "ENCOURS_RETARD_SUP90", "ANCIEN_RELATION_G_RISQUE",
            "INDIC_PERS_INTERDIT_BANC", "Regroup_CSP_Initiale", "CAPACITE_JURIDIQUE", "Categ_NAF_Pro_Agri",
            "TOP_SEUIL_New_def", "DETTES_CT_DETTES_TOT_TIERS", "DETTES_TOT_FLUX_CRED_TIERS", "DETTES_TOT_FLUX_CRED_PRO",
            "NB_MOIS_CREATION_ENTREP", "segment", "incident"]

    categorical = ["Categ_NAF_Pro_Agri", "CRED_Null_Group_bien_fin_Conso", "CRED_Null_Group_interv_tiers",
                   "Regroup_CSP_Initiale", "CAPACITE_JURIDIQUE", "TOP_SEUIL_New_def", "segment", "incident"]

    # Avec la colonne qui donne le segment
    data = pd.read_pickle(r"N:\Projets02\GRO_STAGES\GRO_STG_2021_09 - Logistic Regression Trees\Code\connexion_Python\dataa_app_seg_inc.pkl")

    X_train, labels, enc, scaler, merged_cat, discret_cat = traitement_train(data[Used + ["Defaut_12_Mois_contagion"]],
                                                                             target="Defaut_12_Mois_contagion",
                                                                             categorical=categorical)
    column_names = {'par_0': 'DAV_Null_EHB_DAV', 'par_1': 'DAV_Null_SLD_MOY_CREDITEUR_M',
                    'par_2': 'DAV_Null_SOLDE_MOYEN_FLUX_12M', 'par_3': 'DAV_Null_SOLDE_MOYEN_M',
                    'par_4': 'DAV_Null_SOLDE_MINIMUM_12M', 'par_5': 'DAV_Null_FLX_DBT_NBOPE_DEB_12',
                    'par_6': 'DAV_Null_MNT_REFUS_PAIEMENT_M', 'par_7': 'DAV_Null_MNT_REFUS_PAIEMT_12M',
                    'par_8': 'DAV_Null_NB_REFUS_PAIEMT_3M', 'par_9': 'DAV_Null_NB_REFUS_PAIEMT_6M',
                    'par_10': 'DAV_Null_NB_REFUS_PAIEMT_M', 'par_11': 'DAV_Null_NBPREL_ORG_FINANC_6M',
                    'par_12': 'DAV_Null_NB_TOT_JOURS_DEP_3M', 'par_13': 'DAV_Null_NB_TOT_JOURS_DEP_M',
                    'par_14': 'DAV_Null_NB_TOT_JOURS_DEP_12M', 'par_15': 'DAV_Null_NB_TOT_JOURS_DEP_6M',
                    'par_16': 'DAV_Null_NB_TOT_PMT_CARTE_12M', 'par_17': 'DAV_Null_NBTOT_JOURS_DEBIT_6M',
                    'par_18': 'DAV_Null_NB_OPE_DEBIT_12M', 'par_19': 'DAV_Null_NB_CARTES_DAV',
                    'par_20': 'DAV_Null_NBJOURS_DPS_DEPASS', 'par_21': 'DAV_Null_CPT_JOURS_CONS_DEP_M',
                    'par_22': 'INTERETS_DAV_FLUX', 'par_23': 'INTERETS_DEBIT_12M_TIERS',
                    'par_24': 'CRED_Null_MAXJOUR_CONS_RET_12M', 'par_25': 'CRED_Null_MNT_TOT_IMPAYE_CON',
                    'par_26': 'CRED_Null_NB_JOURS_MAX_RETARD', 'par_27': 'CRED_Null_NB_JOURS_CONS_RETARD',
                    'par_28': 'ENGAGEMENTS_HORS_BILAN_GR_Calc', 'par_29': 'EPARGNE_TOTALE',
                    'par_30': 'EPARGNE_LOGEMENT_GR', 'par_31': 'EPARGNE_LIVRET_GR',
                    'par_32': 'ENCOURS_RETARD_INF90', 'par_33': 'ENCOURS_RETARD_SUP90',
                    'par_34': 'ANCIEN_RELATION_G_RISQUE', 'par_35': 'INDIC_PERS_INTERDIT_BANC',
                    'par_36': 'Regroup_CSP_Initiale', 'par_37': 'CAPACITE_JURIDIQUE', 'par_38': 'Categ_NAF_Pro_Agri',
                    'par_39': 'TOP_SEUIL_New_def', 'par_40': 'DETTES_CT_DETTES_TOT_TIERS',
                    'par_41': 'DETTES_TOT_FLUX_CRED_TIERS', 'par_42': 'DETTES_TOT_FLUX_CRED_PRO',
                    'par_43': 'NB_MOIS_CREATION_ENTREP', 'par_44': 'segment', 'par_45': 'incident'}

    y = data["Defaut_12_Mois_contagion"].replace(["N", "O"], [0, 1])
    y_train = y.astype(np.int32)
    model = Lrtree(criterion="gini", algo='SEM', class_num=8,
                   max_iter=200, validation=True, data_treatment=False)
    model.fit(X=X_train, y=y_train, optimal_size=True, tree_depth=3)
    # model = _fit_parallelized(X=X_train, y=y_train, criterion="gini", algo='SEM', nb_init=8, tree_depth=3, class_num=8,
    #                           max_iter=200, validation=True, data_treatment=False, optimal_size=True, nb_jobs=2)

    logger.info("Régression logistique")
    modele_regLog = linear_model.LogisticRegression(random_state=0, solver='liblinear', multi_class='auto',
                                                    max_iter=100)
    modele_regLog.fit(X_train, y_train)

    logger.info("Arbre de décision")
    model_tree = DecisionTreeClassifier(min_samples_leaf=500, random_state=0)
    model_tree.fit(X_train, y_train)

    logger.info("Gradient Boosting")
    model_boost = GradientBoostingClassifier(min_samples_leaf=100, random_state=0)
    model_boost.fit(X_train, y_train)

    logger.info("Random forest")
    model_forest = RandomForestClassifier(n_estimators=500, min_samples_leaf=100, random_state=0)
    model_forest.fit(X_train, y_train)

    print("Totalité des données de validation")
    table_val = ["LK1ASASVIEW.agri_no_inc_val", "LK1ASASVIEW.agri_inc_val", "LK1ASASVIEW.asso_val",
                 "LK1ASASVIEW.part_inc_val", "LK1ASASVIEW.part_no_inc_val", "LK1ASASVIEW.pp_inc_val",
                 "LK1ASASVIEW.pp_no_inc_val", "LK1ASASVIEW.pro_inc_val", "LK1ASASVIEW.pro_no_inc_val"]
    # segment = ["Agri_no_inc", "Agri_inc", "Asso", "Part_inc", "Part_no_inc", "Pp_inc", "Pp_no_inc", "Pro_inc", "Pro_no_inc"]
    segment = ["Agri", "Agri", "Asso", "Part", "Part", "Pp", "Pp", "Pro", "Pro"]
    incident = [0, 1, 0, 1, 0, 1, 0, 1, 0]

    y_total_train = []
    y_total_proba = []
    y_total_proba_reg = []
    y_total_proba_tree = []
    y_total_proba_boost = []
    y_total_proba_forest = []

    for i in range(len(table_val)):
        table = table_val[i]
        print(table)
        vdf = vDataFrame(table)
        n = len(vdf)
        if n == 0:
            continue
        k = 0
        y_train = []
        y_proba = []
        y_proba_reg = []
        y_proba_tree = []
        y_proba_boost = []
        y_proba_forest = []

        while k + 10000 < n:
            columns = Used + ["Defaut_12_Mois_contagion"]
            data_val_part = vdf.iloc(limit=10000, offset=k, columns=columns).to_pandas()
            data_val_part["segment"] = segment[i]
            data_val_part["incident"] = incident[i]
            k = k + 10000
            X_test = traitement_val(data_val_part[Used + ["Defaut_12_Mois_contagion"]],
                                    enc, scaler, merged_cat, discret_cat)
            y_train = [*y_train,
                       *data_val_part["Defaut_12_Mois_contagion"].replace(["N", "O"], [0, 1]).astype(np.int32)]
            y_proba = [*y_proba, *model.predict_proba(X_test)]
            proba = modele_regLog.predict_proba(X_test)
            y_proba_reg = [*y_proba_reg, *[proba[i][1] for i in range(len(proba))]]
            proba = model_tree.predict_proba(X_test)
            y_proba_tree = [*y_proba_tree, *[proba[i][1] for i in range(len(proba))]]
            proba = model_boost.predict_proba(X_test)
            y_proba_boost = [*y_proba_boost, *[proba[i][1] for i in range(len(proba))]]
            proba = model_forest.predict_proba(X_test)
            y_proba_forest = [*y_proba_forest, *[proba[i][1] for i in range(len(proba))]]

        y_total_train = [*y_total_train, *y_train]
        y_total_proba = [*y_total_proba, *y_proba]
        y_total_proba_reg = [*y_total_proba_reg, *y_proba_reg]
        y_total_proba_tree = [*y_total_proba_tree, *y_proba_tree]
        y_total_proba_boost = [*y_total_proba_boost, *y_proba_boost]
        y_total_proba_forest = [*y_total_proba_forest, *y_proba_forest]

        print("SEM : ", roc_auc_score(y_train, y_proba))
        print("Reglog : ", roc_auc_score(y_train, y_proba_reg))
        print("Tree : ", roc_auc_score(y_train, y_proba_tree))
        print("Boost : ", roc_auc_score(y_train, y_proba_boost))
        print("Forest : ", roc_auc_score(y_train, y_proba_forest))

    print("SEM total : ", roc_auc_score(y_total_train, y_total_proba))
    print("Reglog total : ", roc_auc_score(y_total_train, y_total_proba_reg))
    print("Tree total : ", roc_auc_score(y_total_train, y_total_proba_tree))
    print("Boost total : ", roc_auc_score(y_total_train, y_total_proba_boost))
    print("Forest total : ", roc_auc_score(y_total_train, y_total_proba_forest))
